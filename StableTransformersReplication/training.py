

'''
Here the training pipeline will be implemented:

Key Questions:
    1) How to efficiently store the replay buffer? (store blocks of sequential moves?) (look into how other memory based RL algs did this)

    2) Since off policy, can easily parallelize (look into how to do this nicely. First get working with single server)
    3) What will we use for exploration? Will just use epsilon greedy
'''







#set up replay buffer



#training (soft-actor critic code will take care of this) Likely need to manipulate it to handle past memory


